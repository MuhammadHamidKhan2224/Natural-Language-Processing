# -*- coding: utf-8 -*-
"""Custumer Review Sentiment Analysis .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16z6Acxx7KRVjvfsQ5aMUg2iQAi2vW3Dk
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('Restaurant_Reviews.tsv', sep='\t')

df.head()

df.drop('Liked', axis=1, inplace=True)
df.head()

df.shape

from wordcloud import WordCloud

import matplotlib.pyplot as plt
text = " ".join(i for i in df.Review)
wordcloud = WordCloud(max_font_size=50,
                      max_words=100,
                      background_color="white").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

for i in range(3):
    print(df['Review'], i)

## Removing Alphanumeric, Numbers

import re
import pandas as pd
# Function to clean text
def clean_review(review):
    review = re.sub(r'\d+', '', review)
    # Remove alphanumeric characters
    review = re.sub(r'\w*\d\w*', '', review)
    # Remove articles (a, an, the)
    review = re.sub(r'\b(a|an|the)\b', '', review, flags=re.IGNORECASE)
    return review
# Apply the cleaning function to the dataframe
df['clean_review'] = df['Review'].apply(lambda x: clean_review(x) if pd.notnull(x) else '')

df.head()

## Stemming

import nltk
from nltk.stem import PorterStemmer
st = PorterStemmer()
df['clean_review'] = df['clean_review'].apply(lambda x: " ".join([st.stem(word) for word in x.split()]))

for i in range(3):
    print(df['clean_review'], i)

## Lemmatization

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
df['clean_review'] = df['clean_review'].apply(lambda x: " ".join([lemmatizer.lemmatize(word) for word in x.split()]))

for i in range(3):
    print(df['clean_review'], i)

## Removing Stop Words

nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
df['clean_review'] = df['clean_review'].apply(lambda x: " ".join([word for word in x.split() if word not in stop_words]))

## Convert to lowercase

import pandas as pd
import nltk
df['clean_review'] = df['clean_review'].apply(lambda x: x.lower())

## Tokkenization

import pandas as pd
import nltk
nltk.download('punkt')
# Assuming df is your DataFrame and 'Review' is your column name
def tokenize(text):
    return nltk.word_tokenize(text)
# Apply tokenization
df['clean_review'] = df['clean_review'].apply(tokenize)

for i in range(3):
    print(df['clean_review'], i)

!pip install vaderSentiment

import nltk
nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd

# Initialize the VADER sentiment analyzer
sid = SentimentIntensityAnalyzer()

# Define a function to calculate sentiment scores
def get_sentiment(text):
    # Join the tokenized text back into a single string
    text = ' '.join(text)
    sentiment = sid.polarity_scores(text)
    return sentiment


# Apply the sentiment analysis
df['sentiment'] = df['clean_review'].apply(get_sentiment)

# Separate the sentiment dictionary into individual columns
sentiment_df = df['sentiment'].apply(pd.Series)

# Calculate average sentiment scores for the entire column
average_sentiment = sentiment_df.mean()

# Convert the average sentiment scores to percentages
average_sentiment_percentage = average_sentiment * 100

# Display the results
print(average_sentiment_percentage)

# Plotting the average sentiment scores
average_sentiment_percentage.plot(kind='bar')
plt.title('Overall Sentiment Scores')
plt.ylabel('Average Score')
plt.xlabel('Sentiment')
plt.xticks(rotation=45)
plt.show()

