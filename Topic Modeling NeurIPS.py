# -*- coding: utf-8 -*-
"""Topic Modeling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ycpeta5ogA249MOKdPov4iF05R0Oa2Na
"""

!pip install pyLDAvis

import pyLDAvis
import pyLDAvis.gensim_models
import pickle
import pandas as pd
import os
import re
from wordcloud import WordCloud
import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import gensim.corpora as corpora
import warnings
warnings.filterwarnings("ignore")

file_path = os.path.join(os.getcwd(), 'NeurIPS2020.csv')
ppr = pd.read_csv(file_path)

ppr.shape

ppr.columns

ppr.head()

ppr = ppr.drop(columns=['Unnamed: 0', 'Title', 'Author', 'URL'])

ppr.columns

# Remove punctuation
ppr['Abstract'] = ppr['Abstract'].map(lambda x: re.sub('[,\.!?]', '', x))

for i in range(10):
  print(ppr['Abstract'][i])

long_string = ','.join(list(ppr['Abstract'].values))
worldcloud = WordCloud(background_color="black", max_words=5000, contour_width=5, contour_color='steelblue',width=1000, height=800 )
worldcloud.generate(long_string)
worldcloud.to_image()

# Remove punctuation from 'Abstract' column
ppr['Abstract'] = ppr['Abstract'].map(lambda x: re.sub('[,\.!?]', '', x))

# Define stop words
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'used', 'using', 'use', 'model', 'one', 'two', 'set'])

# Function to tokenize sentences into words
def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

# Function to remove stopwords
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc))
             if word not in stop_words] for doc in texts]

# Process the text data
data = ppr['Abstract'].values.tolist()  # Use the 'Abstract' column from ppr DataFrame
data_words = list(sent_to_words(data))  # Tokenize the text into words

# Remove stopwords
data_words_nostops = remove_stopwords(data_words)

# Print the first 30 words from the first processed document
print(data_words[:1][0][:200])

id2word = corpora.Dictionary(data_words)

texts = data_words

corpus = [id2word.doc2bow(text) for text in texts]

print(corpus[:1][0][:30])

id2word

from pprint import pprint

num_topics = 10

lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics)

pprint(lda_model.print_topics())

doc_lda = lda_model[corpus]

import os
import pickle
import pyLDAvis
import pyLDAvis.gensim_models
import gensim

# Define master_path and ensure the directory exists
master_path = '/path/to/your/project'  # Replace with your actual directory path

if not os.path.exists(master_path):
    os.makedirs(master_path)

# Number of topics for LDA
num_topics = 10  # Set your desired number of topics

# Prepare LDAvis data filepath
LDAvis_data_filepath = os.path.join(master_path, '02_Results', 'ldavis_prepared_' + str(num_topics))

# Ensure the directory for results exists
results_dir = os.path.dirname(LDAvis_data_filepath)
if not os.path.exists(results_dir):
    os.makedirs(results_dir)

# Prepare LDA visualization
LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)

# Save the prepared LDAvis data to a file
with open(LDAvis_data_filepath, 'wb') as f:
    pickle.dump(LDAvis_prepared, f)

# Load the LDAvis data
with open(LDAvis_data_filepath, 'rb') as f:
    LDAvis_prepared = pickle.load(f)

# Save the visualization to an HTML file
pyLDAvis.save_html(LDAvis_prepared, 'ldavis_prepared_' + str(num_topics) + '.html')

# Display the visualization in the notebook
LDAvis_prepared

# The End of the Project